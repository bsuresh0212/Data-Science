Start by installing the packages in the Final folder :--------------------------------------------------------$ pip install -r requirements.txt (if you use python3 means pip3 instead of pip)
For the baseline install fastText :$ git clone https://github.com/facebookresearch/fastText.git$ cd fastText$ make
Then put the files fasttext_tuning.py and files_creation_fasttext.py in the folder fastText You can then use fasttext_tuning.py to optimize the parameters after creating the files for the input of fastText.
copy the file fasttext_tuning.py to fastText folder-----------------------------------------------------https://github.com/Wronskia/Sentiment-Analysis-on-Twitter-data/tree/master/baseline  --> fasttext_tuning.py (PATH)execute in cmd line in fastText folder (ex : example@example/fastText/):: $ python3 fasttext_tuning.py
You can create the input of fasttext by the following :copy the file files_creation_fasttext.py to fastText folder----------------------------------------------------https://github.com/Wronskia/Sentiment-Analysis-on-Twitter-data/tree/master/baseline  --> files_creation_fasttext.py (PATH)execute in cmd line in fastText folder (ex : example@example/fastText/):: $ python3 files_creation_fasttext.py
if(above steps are success){ Put the file files_creation_fasttext.py along with both train_pos.txt and train_neg.txt And by running this command below : $ python3 files_creation_fasttext.py To run the final model : We stored all the features of the 10 models in the folder features. To run the models using the pickled features we provide :
 $ python3 run.py  This will yield our Kaggle prediction that scored 0.88300.
 We used a g2.2xlarge instance on Amazon Web Service. Actually, we runned each one of the 10 models apart (for sake of memory) and we pickled the resulting features.
 However, if you want to run your personalized model or run the models from the start, please :
 run one model at a time by commenting the 9 others in the file models.py and dump the features. For models which uses the first set of features (1-gram + pretrained GloVe), please download the twitter version of GloVe in : http://nlp.stanford.edu/projects/glove/ and put the dezipped file in folder named embeddings contained in the Final folder. After dumping all the features, load them and run XGBoost on the probability matrix (by means of run.py ).}else{ contact saran}
